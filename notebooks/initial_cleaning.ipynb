{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4818d4e5-fba4-449e-9bf8-a3682a394d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import unary_union\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aedb94-f350-460f-a9fc-339c2ba7e328",
   "metadata": {},
   "source": [
    "The first step is to load all of the datasets into dataframes for an initial inspection. Things to looks at include:\n",
    "1. Do the same columns exist across all datasets?\n",
    "2. Do duplicate columns exist across the datasets?\n",
    "3. Where are the null values and how should they be managed?\n",
    "4. Do data types need to be modified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ac2688-68b0-4f67-9dbc-531789985647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree_id</th>\n",
       "      <th>block_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tree_dbh</th>\n",
       "      <th>stump_diam</th>\n",
       "      <th>curb_loc</th>\n",
       "      <th>status</th>\n",
       "      <th>health</th>\n",
       "      <th>spc_latin</th>\n",
       "      <th>spc_common</th>\n",
       "      <th>...</th>\n",
       "      <th>st_assem</th>\n",
       "      <th>st_senate</th>\n",
       "      <th>nta</th>\n",
       "      <th>nta_name</th>\n",
       "      <th>boro_ct</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>x_sp</th>\n",
       "      <th>y_sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>606945</td>\n",
       "      <td>305778</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>Fraxinus pennsylvanica</td>\n",
       "      <td>green ash</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>QN37</td>\n",
       "      <td>Kew Gardens Hills</td>\n",
       "      <td>4125700</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.724339</td>\n",
       "      <td>-73.805180</td>\n",
       "      <td>1.038250e+06</td>\n",
       "      <td>203232.9417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160321</td>\n",
       "      <td>341273</td>\n",
       "      <td>2015-08-19</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>Gleditsia triacanthos var. inermis</td>\n",
       "      <td>honeylocust</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>QN28</td>\n",
       "      <td>Jackson Heights</td>\n",
       "      <td>4030902</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.756626</td>\n",
       "      <td>-73.894167</td>\n",
       "      <td>1.013571e+06</td>\n",
       "      <td>214953.6472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>541347</td>\n",
       "      <td>325281</td>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>Pyrus calleryana</td>\n",
       "      <td>Callery pear</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>QN76</td>\n",
       "      <td>Baisley Park</td>\n",
       "      <td>4028800</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.679777</td>\n",
       "      <td>-73.788463</td>\n",
       "      <td>1.042923e+06</td>\n",
       "      <td>187008.2671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>613930</td>\n",
       "      <td>203822</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>Pyrus calleryana</td>\n",
       "      <td>Callery pear</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>22</td>\n",
       "      <td>BK31</td>\n",
       "      <td>Bay Ridge</td>\n",
       "      <td>3005000</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.622743</td>\n",
       "      <td>-74.037543</td>\n",
       "      <td>9.738279e+05</td>\n",
       "      <td>166160.5847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18353</td>\n",
       "      <td>338911</td>\n",
       "      <td>2015-06-13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>OnCurb</td>\n",
       "      <td>Alive</td>\n",
       "      <td>Good</td>\n",
       "      <td>Prunus virginiana</td>\n",
       "      <td>'Schubert' chokecherry</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>QN12</td>\n",
       "      <td>Hammels-Arverne-Edgemere</td>\n",
       "      <td>4095400</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.596514</td>\n",
       "      <td>-73.797622</td>\n",
       "      <td>1.040452e+06</td>\n",
       "      <td>156667.5017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tree_id  block_id  created_at  tree_dbh  stump_diam curb_loc status health  \\\n",
       "0   606945    305778  2016-06-28        10           0   OnCurb  Alive   Good   \n",
       "1   160321    341273  2015-08-19         9           0   OnCurb  Alive   Good   \n",
       "2   541347    325281  2015-12-30         7           0   OnCurb  Alive   Good   \n",
       "3   613930    203822  2016-07-05        10           0   OnCurb  Alive   Good   \n",
       "4    18353    338911  2015-06-13         4           0   OnCurb  Alive   Good   \n",
       "\n",
       "                            spc_latin              spc_common  ... st_assem  \\\n",
       "0              Fraxinus pennsylvanica               green ash  ...       25   \n",
       "1  Gleditsia triacanthos var. inermis             honeylocust  ...       34   \n",
       "2                    Pyrus calleryana            Callery pear  ...       32   \n",
       "3                    Pyrus calleryana            Callery pear  ...       46   \n",
       "4                   Prunus virginiana  'Schubert' chokecherry  ...       31   \n",
       "\n",
       "  st_senate   nta                  nta_name  boro_ct     state   latitude  \\\n",
       "0        14  QN37         Kew Gardens Hills  4125700  New York  40.724339   \n",
       "1        13  QN28           Jackson Heights  4030902  New York  40.756626   \n",
       "2        10  QN76              Baisley Park  4028800  New York  40.679777   \n",
       "3        22  BK31                 Bay Ridge  3005000  New York  40.622743   \n",
       "4        10  QN12  Hammels-Arverne-Edgemere  4095400  New York  40.596514   \n",
       "\n",
       "   longitude          x_sp         y_sp  \n",
       "0 -73.805180  1.038250e+06  203232.9417  \n",
       "1 -73.894167  1.013571e+06  214953.6472  \n",
       "2 -73.788463  1.042923e+06  187008.2671  \n",
       "3 -74.037543  9.738279e+05  166160.5847  \n",
       "4 -73.797622  1.040452e+06  156667.5017  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tree census data into a dataframe and display the first 5 rows\n",
    "path_2015 = \"../data/raw/tree_data/new_york_tree_census_2015.csv\"\n",
    "df = pd.read_csv(path_2015)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd7aedb-4174-4fce-963b-f70ed2a16cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the custom functions will be stored in this cell for later use\n",
    "\n",
    "def trim_and_lower(df):\n",
    "  df2 = df.copy()\n",
    "\n",
    "  df2.columns = [column.strip() for column in df2.columns]\n",
    "  df2.columns = [column.strip().replace(\" \", \"_\").lower() for column in df2.columns]\n",
    "\n",
    "  df2 = df2.map(lambda x: x.replace(\"\\xa0\", \" \") if isinstance(x, str) else x)\n",
    "  df2 = df2.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "  df2 = df2.map(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "  return df2\n",
    "\n",
    "\n",
    "def summary(df, cat_threshold=10, format_check=None):\n",
    "    categorical = {}\n",
    "    numerical = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        key = col\n",
    "        value = df[col].nunique()\n",
    "        labels = list(df[col].unique())\n",
    "        if value <= cat_threshold:\n",
    "            categorical.update({key: labels})\n",
    "        else:\n",
    "            numerical.update({key: value})\n",
    "\n",
    "    nulls = null_columns(df)\n",
    "\n",
    "    print(f\"Columns containing unique values below or equal to the given threshold of {cat_threshold}:\\n\")\n",
    "    for key, value in categorical.items():\n",
    "        print(key,\":\", value)\n",
    "\n",
    "    print(f\"\\n\\nColumns containing unique values above the given threshold of {cat_threshold}:\\n\")\n",
    "    for key, value in numerical.items():\n",
    "        print(key,\":\", value)\n",
    "\n",
    "    print(\"\\n\\nColumns with NULL values:\\n\")\n",
    "    for key, value in nulls.items():\n",
    "        print(key,\":\", value)\n",
    "\n",
    "    print(\"\\n\\nIrregular format values:\\n\")\n",
    "    if format_check:\n",
    "        for col in format_check:\n",
    "            len_set = set()\n",
    "            df['length'] = df[col].astype(\"str\").apply(len)\n",
    "            min_len = df[\"length\"].min()\n",
    "            max_len = df[\"length\"].max()\n",
    "            len_set.update([min_len, max_len])\n",
    "            if len(len_set) > 1:\n",
    "                print(col, \":\", len_set)\n",
    "            df.drop(columns=\"length\", inplace=True)\n",
    "    else:\n",
    "        print(\"None\")\n",
    "\n",
    "    \n",
    "\n",
    "def null_columns(df):\n",
    "    null_cols = df.columns[df.isnull().any()]\n",
    "    df_null_cols = df[null_cols]\n",
    "    return df_null_cols.isna().sum()\n",
    "\n",
    "\n",
    "def compare_cols(df_list):\n",
    "    cols_set = set()\n",
    "    my_dict = {}\n",
    "\n",
    "    for df in df_list:\n",
    "        cols = df.columns\n",
    "        cols_set.update(cols)\n",
    "\n",
    "    for df in df_list:\n",
    "        col_name = df.attrs[\"name\"]\n",
    "        search_result = []\n",
    "        for item in cols_set:\n",
    "            if item in df.columns:\n",
    "                search_result.append(True)\n",
    "            else:\n",
    "                search_result.append(False)\n",
    "        my_dict.update({col_name: search_result})\n",
    "\n",
    "    new_df = pd.DataFrame(my_dict, index=(list(cols_set)))\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def rename_columns(df_list, col_map):\n",
    "    df_list_new = []\n",
    "    for df in df_list:\n",
    "        df = df.rename(columns=col_map)\n",
    "        df_list_new.append(df)\n",
    "\n",
    "    return df_list_new\n",
    "\n",
    "\n",
    "def cols_set(df_list):\n",
    "    cols_set = set()\n",
    "\n",
    "    for df in df_list:\n",
    "        cols = list(df.columns.sort_values())\n",
    "        cols_set.update(cols)\n",
    "\n",
    "    return cols_set\n",
    "\n",
    "\n",
    "def get_locations_data_nearest(df, epsg=32618, max_distance=1800): # 4326\n",
    "    \n",
    "    # Load the geo data\n",
    "    path_to_data = \"../data/raw/geo_data/MODZCTA.geojson\"\n",
    "    gdf = gpd.read_file(path_to_data)[[\"modzcta\", \"geometry\", \"pop_est\"]]\n",
    "\n",
    "    # Filter out the row where modzcta == \"99999\"\n",
    "    gdf = gdf.loc[gdf['modzcta'] != '99999']\n",
    "    \n",
    "    # Assign the new name to the chosen dataset\n",
    "    locations_df = df.reset_index(drop=True)\n",
    "    print(\"The shape of locations_df is:\", locations_df.shape)\n",
    "    \n",
    "    # Convert locations_df to a GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(locations_df['longitude'], locations_df['latitude'])]\n",
    "    print(\"The shape of geometry is:\", len(geometry))\n",
    "    \n",
    "    locations_gdf = gpd.GeoDataFrame(locations_df, geometry=geometry).reset_index(drop=True)\n",
    "    print(\"The shape of locations_gdf is:\", locations_gdf.shape)\n",
    "    print(\"The CRS for locations_gdf is:\", locations_gdf.crs)\n",
    "    \n",
    "    # Ensure both GeoDataFrames use the same coordinate reference system (CRS)\n",
    "    locations_gdf.set_crs(epsg=4326, inplace=True)  # Assuming WGS84 CRS\n",
    "    locations_gdf.to_crs(epsg=epsg, inplace=True)  # Assuming WGS84 CRS\n",
    "    print(\"The CRS for locations_gdf is:\", locations_gdf.crs)\n",
    "    gdf.to_crs(epsg=epsg, inplace=True)\n",
    "    print(\"The CRS for gdf is:\", locations_gdf.crs)\n",
    "    \n",
    "    # Perform spatial join\n",
    "    joined_gdf = gpd.sjoin_nearest(locations_gdf, gdf, how='left', max_distance=max_distance)\n",
    "    print(\"The shape of joined_gdf is:\", joined_gdf.shape)\n",
    "\n",
    "    # Drop duplicates that may have been created during the spatial join\n",
    "    joined_gdf_unique = joined_gdf.drop_duplicates(subset=[\"tree_id\"], keep=\"first\")\n",
    "    print(\"The shape of joined_gdf_unique is:\", joined_gdf_unique.shape)\n",
    "\n",
    "    # Count null values in new \"ZCTA5CE20\" column\n",
    "    nulls = joined_gdf_unique[\"modzcta\"].isna().sum()\n",
    "    print(\"The number of rows with null values for modzcta is:\", nulls)\n",
    "    print(\"\\n****************************************\\n\")\n",
    "\n",
    "    joined_gdf_unique = joined_gdf_unique.drop(columns=\"index_right\").copy()\n",
    "\n",
    "    return joined_gdf_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7933c4-673a-4db4-95fb-d84c37bfd5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(683788, 41)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc20196-0085-4a02-a07c-1c5de8a01a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tree_id            0\n",
       "block_id           0\n",
       "created_at         0\n",
       "tree_dbh           0\n",
       "stump_diam         0\n",
       "curb_loc           0\n",
       "status             0\n",
       "health         31616\n",
       "spc_latin      31619\n",
       "spc_common     31619\n",
       "steward       519438\n",
       "guards        603922\n",
       "sidewalk       31616\n",
       "user_type          0\n",
       "problems      457944\n",
       "root_stone         0\n",
       "root_grate         0\n",
       "root_other         0\n",
       "trunk_wire         0\n",
       "trnk_light         0\n",
       "trnk_other         0\n",
       "brch_light         0\n",
       "brch_shoe          0\n",
       "brch_other         0\n",
       "address            0\n",
       "zipcode            0\n",
       "zip_city           0\n",
       "cb_num             0\n",
       "borocode           0\n",
       "boroname           0\n",
       "cncldist           0\n",
       "st_assem           0\n",
       "st_senate          0\n",
       "nta                0\n",
       "nta_name           0\n",
       "boro_ct            0\n",
       "state              0\n",
       "latitude           0\n",
       "longitude          0\n",
       "x_sp               0\n",
       "y_sp               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea659fa4-b0a4-4679-bc82-a439791c515a",
   "metadata": {},
   "source": [
    "## Fix 2015 dataset\n",
    "- fill \"nan\" values in the \"health\" column with \"dead\" or \"stump\"\n",
    "- \"status\" and \"health\" need to be standardised in the 2005 and 2015 datasets\n",
    "- \"steward\" \"nan\" should be \"none\" and other values should be changed to \"low\", \"medium\" and \"high\" (not recorded for dead or stumps)\n",
    "- fill \"nan\" values with \"none\" for \"guards\" (not recorded for dead or stumps)\n",
    "- \"sidewalk_damage\" (yes / no): (not recorded for dead or stumps)\n",
    "- some zip codes are of an irregular format, but it may be possible to predict them based on the lat. long. values, which are all correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5467a0ed-27ea-423e-8d6e-1f3b61aebb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns containing unique values below or equal to the given threshold of 20:\n",
      "\n",
      "curb_loc : ['oncurb', 'offsetfromcurb']\n",
      "status : ['alive', 'dead', 'stump']\n",
      "health : ['good', 'fair', nan, 'poor']\n",
      "steward : [nan, '1or2', '3or4', '4ormore']\n",
      "guards : [nan, 'helpful', 'harmful', 'unsure']\n",
      "sidewalk : ['nodamage', 'damage', nan]\n",
      "user_type : ['treescount staff', 'volunteer', 'nyc parks staff']\n",
      "root_stone : ['yes', 'no']\n",
      "root_grate : ['no', 'yes']\n",
      "root_other : ['no', 'yes']\n",
      "trunk_wire : ['no', 'yes']\n",
      "trnk_light : ['no', 'yes']\n",
      "trnk_other : ['no', 'yes']\n",
      "brch_light : ['no', 'yes']\n",
      "brch_shoe : ['no', 'yes']\n",
      "brch_other : ['no', 'yes']\n",
      "borocode : [np.int64(4), np.int64(3), np.int64(1), np.int64(2), np.int64(5)]\n",
      "boroname : ['queens', 'brooklyn', 'manhattan', 'bronx', 'staten island']\n",
      "state : ['new york']\n",
      "\n",
      "\n",
      "Columns containing unique values above the given threshold of 20:\n",
      "\n",
      "tree_id : 683788\n",
      "block_id : 101390\n",
      "created_at : 483\n",
      "tree_dbh : 146\n",
      "stump_diam : 100\n",
      "spc_latin : 132\n",
      "spc_common : 132\n",
      "problems : 231\n",
      "address : 408701\n",
      "zipcode : 191\n",
      "zip_city : 48\n",
      "cb_num : 59\n",
      "cncldist : 51\n",
      "st_assem : 65\n",
      "st_senate : 26\n",
      "nta : 188\n",
      "nta_name : 188\n",
      "boro_ct : 2152\n",
      "latitude : 676080\n",
      "longitude : 677101\n",
      "x_sp : 681630\n",
      "y_sp : 682632\n",
      "\n",
      "\n",
      "Columns with NULL values:\n",
      "\n",
      "health : 31616\n",
      "spc_latin : 31619\n",
      "spc_common : 31619\n",
      "steward : 519438\n",
      "guards : 603922\n",
      "sidewalk : 31616\n",
      "problems : 457944\n",
      "\n",
      "\n",
      "Irregular format values:\n",
      "\n",
      "longitude : {np.int64(12), np.int64(7)}\n",
      "latitude : {np.int64(11), np.int64(5)}\n",
      "zipcode : {np.int64(2), np.int64(5)}\n"
     ]
    }
   ],
   "source": [
    "# Trim whitespace and convert all strings to lower case\n",
    "df = trim_and_lower(df)\n",
    "\n",
    "# Print a summary of the dataset using a custom function\n",
    "summary(df, cat_threshold=20, format_check=[\"longitude\", \"latitude\", \"zipcode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59829496-af8b-45b7-940c-0470d99f12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on the unique tree_id\n",
    "df = df.drop_duplicates(subset=\"tree_id\", keep='first').copy()\n",
    "\n",
    "# Combine 'dead' and 'stump' into one value in the health column and fill null values with the mode\n",
    "df[\"health\"] = df.apply(lambda x: \"dead / stump\" if x[\"status\"] in [\"dead\", \"stump\"] else x[\"health\"], axis=1).fillna(df[\"health\"].mode()[0])\n",
    "\n",
    "# Replace the value for steward with 'not applicable' if the tree is 'dead' or 'stump' and fill nulls values with 'none'\n",
    "df[\"steward\"] = df.apply(lambda x: \"not applicable\" if x[\"status\"] in [\"dead\", \"stump\"] else x[\"steward\"], axis=1).fillna(\"none\")\n",
    "\n",
    "# Do the same for guards, sidewalk_damage, spc_common and spc_latin as is done for steward\n",
    "df[\"guards\"] = df.apply(lambda x: \"not applicable\" if x[\"status\"] in [\"dead\", \"stump\"] else x[\"guards\"], axis=1).fillna(\"none\")\n",
    "df[\"sidewalk_damage\"] = df.apply(lambda x: \"not applicable\" if x[\"status\"] in [\"dead\", \"stump\"] else (\"yes\" if x[\"sidewalk\"] == \"damage\" else \"no\"), axis=1)\n",
    "df[\"spc_common\"] = df.apply(lambda x: \"not applicable\" if x[\"status\"] in [\"dead\", \"stump\"] else x[\"spc_common\"], axis=1)\n",
    "df[\"spc_latin\"] = df.apply(lambda x: \"not applicable\" if x[\"status\"] in [\"dead\", \"stump\"] else x[\"spc_latin\"], axis=1)\n",
    "\n",
    "# Combine columns into simplified, single columns called ground_level_conflict and tree_level_conflict\n",
    "ground_level_conflict_zipped = zip(df[\"root_stone\"], df[\"root_grate\"], df[\"root_other\"])\n",
    "df[\"ground_level_conflict\"] = [\"yes\" if any(row == \"yes\" for row in values) else \"no\" for values in ground_level_conflict_zipped]\n",
    "tree_level_conflict_zipped = zip(df[\"trnk_light\"], df[\"trnk_other\"], df[\"brch_light\"], df[\"brch_shoe\"], df[\"brch_other\"], df[\"trunk_wire\"])\n",
    "df[\"tree_level_conflict\"] = [\"yes\" if any(row == \"yes\" for row in values) else \"no\" for values in tree_level_conflict_zipped]\n",
    "\n",
    "# Replace existing values in the stewardship column with more descriptive values\n",
    "steward_replacements = {\n",
    "    \"1or2\": \"low\",\n",
    "    \"3or4\": \"medium\",\n",
    "    \"4ormore\": \"high\"\n",
    "}\n",
    "df[\"steward\"] = df[\"steward\"].replace(steward_replacements)\n",
    "\n",
    "# Rename the boroname column to borough\n",
    "df.rename(columns={'boroname': 'borough'}, inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cols_to_keep = [\n",
    "    'ground_level_conflict',\n",
    "    'guards',\n",
    "    'health',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'sidewalk_damage',\n",
    "    'spc_common',\n",
    "    'steward',\n",
    "    'tree_dbh',\n",
    "    'tree_id',\n",
    "    'tree_level_conflict',\n",
    "    \"nta\",\n",
    "    \"nta_name\",\n",
    "    \"borough\"\n",
    "]\n",
    "\n",
    "cols_to_drop = [col for col in df.columns if col not in cols_to_keep]\n",
    "df = df.drop(columns=cols_to_drop).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6737a921-8471-49af-8f56-ab614a909e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of locations_df is: (683788, 14)\n",
      "The shape of geometry is: 683788\n",
      "The shape of locations_gdf is: (683788, 15)\n",
      "The CRS for locations_gdf is: None\n",
      "The CRS for locations_gdf is: EPSG:32618\n",
      "The CRS for gdf is: EPSG:32618\n",
      "The shape of joined_gdf is: (683788, 18)\n",
      "The shape of joined_gdf_unique is: (683788, 18)\n",
      "The number of rows with null values for modzcta is: 0\n",
      "\n",
      "****************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieve zip code and population estimate from the MODZCTA.geojson file\n",
    "df = get_locations_data_nearest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a416a0-2ec7-43c3-bb0f-90ec3cb6fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_map = {\n",
    "    'diameter': \"tree_diameter\",\n",
    "    'guards': \"guards_impact\",\n",
    "    'health': \"tree_condition\",\n",
    "    'spc_common': \"species_common_name\",\n",
    "    'steward': \"stewardship_level\",\n",
    "    'tree_dbh': \"tree_diameter\",\n",
    "    'modzcta': \"zip_code\",\n",
    "    'nta': \"nta_code\"\n",
    "}\n",
    "\n",
    "df = df.rename(columns=col_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f0ab72-ed8a-4141-bb42-685010308f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tree_id', 'tree_diameter', 'tree_condition', 'species_common_name',\n",
       "       'stewardship_level', 'guards_impact', 'borough', 'nta_code', 'nta_name',\n",
       "       'latitude', 'longitude', 'sidewalk_damage', 'ground_level_conflict',\n",
       "       'tree_level_conflict', 'geometry', 'zip_code', 'pop_est'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a52e341-91a6-4595-b3a0-beca3e45094a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tree_id                  0\n",
       "tree_diameter            0\n",
       "tree_condition           0\n",
       "species_common_name      5\n",
       "stewardship_level        0\n",
       "guards_impact            0\n",
       "borough                  0\n",
       "nta_code                 0\n",
       "nta_name                 0\n",
       "latitude                 0\n",
       "longitude                0\n",
       "sidewalk_damage          0\n",
       "ground_level_conflict    0\n",
       "tree_level_conflict      0\n",
       "geometry                 0\n",
       "zip_code                 0\n",
       "pop_est                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb46446-a55d-4bd1-a37d-43a6bd0c909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a filtered dataframe to fit the LabelEncoder\n",
    "df_notna = df.loc[df.species_common_name.notna(), [\"longitude\", \"latitude\", \"species_common_name\"]]\n",
    "\n",
    "# Initialize LabelEncoder and encode non-null values\n",
    "le = LabelEncoder()\n",
    "df_notna.species_common_name = le.fit_transform(df_notna.species_common_name)\n",
    "\n",
    "# Update the original dataframe with the encoded values\n",
    "df.update(df_notna.species_common_name)\n",
    "\n",
    "# Initialize the KNNImputer and impute the missing values using the encoded values\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "imputed_data = imputer.fit_transform(df[[\"longitude\", \"latitude\", \"species_common_name\"]])\n",
    "\n",
    "# Assign the imputed values back to species_common_name in the main dataframe\n",
    "df['species_common_name'] = imputed_data[:, 2].astype(int)\n",
    "\n",
    "# Transform the encoded values back to the original labels\n",
    "df.species_common_name = le.inverse_transform(df.species_common_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5fe72f9-f3c7-471e-9851-6df211e119b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tree_id                  0\n",
       "tree_diameter            0\n",
       "tree_condition           0\n",
       "species_common_name      0\n",
       "stewardship_level        0\n",
       "guards_impact            0\n",
       "borough                  0\n",
       "nta_code                 0\n",
       "nta_name                 0\n",
       "latitude                 0\n",
       "longitude                0\n",
       "sidewalk_damage          0\n",
       "ground_level_conflict    0\n",
       "tree_level_conflict      0\n",
       "geometry                 0\n",
       "zip_code                 0\n",
       "pop_est                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "722b90d3-ff5e-4f5e-9a35-52c1fa598ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "#df.drop(columns=\"index_right\", inplace=True)\n",
    "\n",
    "df.to_csv(\"../data/clean/tree_data/2015.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac94bb-1a36-47bc-820e-f139733aa6c5",
   "metadata": {},
   "source": [
    "#### Modify GEOJSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e97b7b-88ff-42aa-99bb-420f12e97017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_multipolygons_with_group(gdf, group_cols):\n",
    "\n",
    "    # Save original CRS\n",
    "    original_crs = gdf.crs\n",
    "\n",
    "    # Group by the 'suburb' column and combine geometries\n",
    "    grouped = gdf.groupby(group_cols)['geometry'].apply(lambda x: unary_union(x))\n",
    "    \n",
    "    # Create a new GeoDataFrame\n",
    "    combined_gdf = gpd.GeoDataFrame(grouped, geometry='geometry').reset_index()\n",
    "\n",
    "    # Ensure the GeoDataFrame has a CRS\n",
    "    if combined_gdf.crs is None:\n",
    "        combined_gdf.set_crs(original_crs, inplace=True)\n",
    "    \n",
    "    return combined_gdf\n",
    "\n",
    "\n",
    "# Load your GeoDataFrame (example using a file)\n",
    "path_to_data = \"../data/raw/geo_data/2010 Census Tracts.geojson\"\n",
    "gdf = gpd.read_file(path_to_data)[[\"ntacode\", \"ntaname\", \"boro_name\", \"geometry\"]]\n",
    "gdf = trim_and_lower(gdf)\n",
    "\n",
    "# List the columns to group by\n",
    "group_cols = [\"ntaname\", \"ntacode\", \"boro_name\"]\n",
    "\n",
    "# Combine MULTIPOLYGON geometries by suburb\n",
    "combined_gdf = combine_multipolygons_with_group(gdf, group_cols)\n",
    "\n",
    "# Convert to the appropriate CRS (was epsg=4326)\n",
    "combined_gdf.to_crs(epsg=32618, inplace=True)\n",
    "\n",
    "# Add area in m2 and km2\n",
    "combined_gdf[\"area_nta_m2\"] = combined_gdf.geometry.area\n",
    "combined_gdf[\"area_nta_km2\"] = combined_gdf[\"area_nta_m2\"] / 1000000\n",
    "combined_gdf[\"area_nta_hectares\"] = combined_gdf[\"area_nta_m2\"] / 10000 # hectares\n",
    "\n",
    "# Add centroid for each MULTIPOLYGON\n",
    "combined_gdf['nta_centroid'] = combined_gdf.geometry.centroid\n",
    "combined_gdf['nta_centroid_lat'] = combined_gdf.nta_centroid.y\n",
    "combined_gdf['nta_centroid_lon'] = combined_gdf.nta_centroid.x\n",
    "\n",
    "new_col_names = {\n",
    "    \"ntaname\": \"nta_name\",\n",
    "    \"ntacode\": \"nta_code\",\n",
    "    \"boro_name\": \"borough\",\n",
    "    \"geometry\": \"nta_geometry\"\n",
    "}\n",
    "combined_gdf.rename(columns=new_col_names, inplace=True)\n",
    "\n",
    "combined_gdf.drop(columns=\"nta_centroid\", inplace=True)\n",
    "\n",
    "# Export data to GeoJSON file\n",
    "combined_gdf.to_file(\"../data/clean/geo_data/nta_data.geojson\", index=False, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2014208d-79a3-4be4-b784-0ecc4c194a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geo data\n",
    "path_to_data = \"../data/raw/geo_data/MODZCTA.geojson\"\n",
    "gdf = gpd.read_file(path_to_data)[[\"modzcta\", \"geometry\"]]\n",
    "\n",
    "# Reproject to the desired CRS for area calculation\n",
    "gdf = gdf.to_crs(epsg=32618) # 3395\n",
    "gdf[\"area_zip_m2\"] = gdf[\"geometry\"].area # meters squared\n",
    "gdf[\"area_zip_km2\"] = gdf[\"area_zip_m2\"] / 1000000 # kilometers squared\n",
    "gdf[\"area_zip_hectares\"] = gdf[\"area_zip_m2\"] / 10000 # hectares\n",
    "\n",
    "# Calculate centroids in the projected CRS (EPSG:3395)\n",
    "gdf['zip_centroid'] = gdf.geometry.centroid\n",
    "\n",
    "# Reproject the centroids to EPSG:4326\n",
    "#gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Extract latitude and longitude from the reprojected centroids\n",
    "gdf['zip_centroid_lat'] = gdf.centroid.y\n",
    "gdf['zip_centroid_lon'] = gdf.centroid.x\n",
    "\n",
    "# Drop the intermediary centroid column if no longer needed\n",
    "new_col_names = {\n",
    "    'geometry': \"zip_geometry\",\n",
    "    \"modzcta\": \"zip_code\"\n",
    "}\n",
    "df_zip_codes = gdf.rename(columns=new_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28a67fb7-ccad-49f0-8f98-bf8d5d717687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip_code               object\n",
       "area_zip_m2           float64\n",
       "area_zip_km2          float64\n",
       "area_zip_hectares     float64\n",
       "zip_centroid           object\n",
       "zip_centroid_lat      float64\n",
       "zip_centroid_lon      float64\n",
       "geometry             geometry\n",
       "nta_name               object\n",
       "nta_code               object\n",
       "borough                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING CELL\n",
    "\n",
    "# Load the geo data for the spatial join\n",
    "gdf = combined_gdf[[\"nta_name\", \"nta_code\", \"borough\", \"nta_geometry\"]].copy()\n",
    "gdf.set_geometry(\"nta_geometry\", inplace=True)\n",
    "\n",
    "# Convert locations_df to a GeoDataFrame\n",
    "#geometry = [Point(xy) for xy in zip(df_zip_codes['zip_centroid_lon'], df_zip_codes['zip_centroid_lat'])]\n",
    "geometry = df_zip_codes.zip_centroid\n",
    "zip_codes_gdf = gpd.GeoDataFrame(df_zip_codes, geometry=geometry)\n",
    "\n",
    "# Ensure both GeoDataFrames use the same coordinate reference system (CRS)\n",
    "zip_codes_gdf.set_crs(epsg=32618, inplace=True)  # Assuming WGS84 CRS\n",
    "gdf = gdf.to_crs(epsg=32618)\n",
    "\n",
    "# Perform spatial join\n",
    "joined_gdf = gpd.sjoin_nearest(zip_codes_gdf, gdf, how='left', max_distance=350.0)\n",
    "joined_gdf_unique = joined_gdf.drop_duplicates(subset=[\"zip_code\"], keep=\"first\")\n",
    "#joined_gdf_unique.set_geometry(\"zip_geometry\", inplace=True)\n",
    "joined_gdf_unique[\"geometry\"] = joined_gdf_unique[\"zip_geometry\"]\n",
    "\n",
    "# Trim all white space and convert to lower case for consistency\n",
    "joined_gdf_unique = trim_and_lower(joined_gdf_unique)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "joined_gdf_unique.drop(columns=[\"index_right\", \"zip_geometry\"], inplace=True)\n",
    "\n",
    "joined_gdf_unique.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62aa28a6-251c-4e87-a57e-dcd658002430",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf_unique.to_file(\"../data/clean/geo_data/zip_data.geojson\", index=False, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30f782f-3279-4ca8-a603-de05a2778296",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_path = \"../data/raw/geo_data/Borough Boundaries.geojson\"\n",
    "gdfb = gpd.read_file(borough_path)\n",
    "\n",
    "# Convert boro_name to lower case and rename to borough\n",
    "gdfb.boro_name = gdfb.boro_name.str.lower()\n",
    "gdfb = gdfb.rename(columns={\"boro_name\": \"borough\"})\n",
    "\n",
    "# Reproject to the desired CRS for area calculation\n",
    "gdfb = gdfb.to_crs(epsg=32618) # 3395\n",
    "gdfb[\"area_borough_m2\"] = gdfb[\"geometry\"].area # meters squared\n",
    "gdfb[\"area_borough_km2\"] = gdfb[\"area_borough_m2\"] / 1000000 # kilometers squared\n",
    "gdfb[\"area_borough_hectares\"] = gdfb[\"area_borough_m2\"] / 10000 # hectares\n",
    "\n",
    "# Drop unnecessary columns\n",
    "gdfb.drop(columns=[\"shape_area\", \"shape_leng\"], inplace=True)\n",
    "\n",
    "gdfb.to_file(\"../data/clean/geo_data/borough_data.geojson\", index=False, driver=\"GeoJSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-tree-census-venv",
   "language": "python",
   "name": "nyc-tree-census-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
